---
title: "Mastodon - 2024-11-20T06:34:07Z"
subtitle: ""
canonical: https://hachyderm.io/users/mweagle/statuses/113513886337695806
description:
image: "/images/mastodon.png"

date: 2024-11-20T06:34:07Z
lastmod: 2024-11-20T06:34:07Z
image: ""
tags: ["Social Media"]

categories: ["mastodon"]
# generated: 2025-05-22T22:29:20-07:00
---
![Mastodon](/images/mastodon.png)

<p>â€œSo this entire process, in order to add â€œwhat is 2+2â€, we do a non-deterministic a lookup from an enormous hashtable that contains the sum of public human knowledge weâ€™ve seen fit to collect for our dataset, then we squeeze it through the tiny, nondeterministic funnels of decoding strategies and guided generation to get to an answer from a sampled probability distribution.â€</p><p><a href="https://newsletter.vickiboykis.com/archive/why-are-we-using-llms-as-calculators/" target="_blank" rel="nofollow noopener noreferrer" translate="no"><span class="invisible">https://</span><span class="ellipsis">newsletter.vickiboykis.com/arc</span><span class="invisible">hive/why-are-we-using-llms-as-calculators/</span></a></p>


###### [Mastodon Source ğŸ˜](https://hachyderm.io/@mweagle/113513886337695806)

___

<p>â€œWe hypothesize that this decline is due to the fact that current LLMs are not capable of genuine logical reasoning; instead, they attempt to replicate the reasoning steps observed in their training data. When we add a single clause that appears relevant to the question, we observe significant performance drops (up to 65%) across all state-of-the-art models, even though the added clause does not contribute to the reasoning chain needed to reach the final answerâ€ </p><p><a href="https://arxiv.org/abs/2410.05229" target="_blank" rel="nofollow noopener noreferrer" translate="no"><span class="invisible">https://</span><span class="">arxiv.org/abs/2410.05229</span><span class="invisible"></span></a></p>


###### [Mastodon Source ğŸ˜](https://hachyderm.io/@mweagle/113513896908279852)

___
